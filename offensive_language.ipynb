{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f88f9c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600cc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import ast\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f82a628",
   "metadata": {},
   "source": [
    "## Proccess data\n",
    "1. **SemEval-2021 Task 5: Toxic Spans Detection**, to detect the offensive part of the message\n",
    "2. **Jigsaw**, to detect toxic messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59878b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for the SemEval-2021 Task 5: Toxic Spans Detection dataset\n",
    "def extract_text_and_spans(dataset_split):\n",
    "    X = []  # texts\n",
    "    y = []  # spans\n",
    "    \n",
    "    for sample in dataset_split:\n",
    "        text = sample[\"text_of_post\"]\n",
    "        X.append(text)\n",
    "        # Parse positions and convert to spans\n",
    "        try:\n",
    "            toxic_positions = ast.literal_eval(sample[\"position\"])\n",
    "        except:\n",
    "            toxic_positions = []\n",
    "        # Convert positions to spans [start, end)\n",
    "        spans = []\n",
    "        if toxic_positions:\n",
    "            toxic_positions = sorted(toxic_positions)\n",
    "            start = toxic_positions[0]\n",
    "            end = toxic_positions[0]\n",
    "            \n",
    "            for pos in toxic_positions[1:]:\n",
    "                if pos == end + 1:  # Consecutive\n",
    "                    end = pos\n",
    "                else:  # Gap found\n",
    "                    spans.append([start, end + 1])\n",
    "                    start = pos\n",
    "                    end = pos\n",
    "            spans.append([start, end + 1])\n",
    "        y.append(spans)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7901fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemEval-2021 Task 5: Toxic Spans Detection\n",
    "dataset = load_dataset(\"heegyu/toxic-spans\")\n",
    "train = dataset[\"train\"]\n",
    "test = dataset[\"test\"]\n",
    "X_train_span, y_train_span = extract_text_and_spans(train)\n",
    "X_test_span, y_test_span = extract_text_and_spans(test)\n",
    "\n",
    "# Jigsaw\n",
    "# -Train / Val-\n",
    "subcategories = [\"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_data = pd.read_csv(\"jigsaw-toxic-comment-data/train.csv\")\n",
    "# If any subcategory is 1, set toxic to 1\n",
    "train_data[\"toxic\"] = train_data[[\"toxic\"] + subcategories].max(axis=1)\n",
    "X_train_toxic = train_data[\"comment_text\"]\n",
    "y_train_toxic = train_data[\"toxic\"]\n",
    "# Split to train and val\n",
    "X_train_toxic, X_val_toxic, y_train_toxic, y_val_toxic = train_test_split(\n",
    "    X_train_toxic, y_train_toxic, test_size=0.15, stratify=y_train_toxic, random_state=2025\n",
    ")\n",
    "\n",
    "# -Test-\n",
    "test_text = pd.read_csv(\"jigsaw-toxic-comment-data/test.csv\")\n",
    "test_labels = pd.read_csv(\"jigsaw-toxic-comment-data/test_labels.csv\")\n",
    "# Keep only rows where toxic is not -1\n",
    "mask = test_labels[\"toxic\"] != -1\n",
    "test_text = test_text[mask].reset_index(drop=True)\n",
    "test_labels = test_labels[mask].reset_index(drop=True)\n",
    "test_labels[\"toxic\"] = test_labels[[\"toxic\"] + subcategories].max(axis=1)\n",
    "X_test_toxic = test_text[\"comment_text\"]\n",
    "y_test_toxic = test_labels[\"toxic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "813e7c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d25607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "# Create datasets # SemEval-2021 Task 5: Toxic Spans Detection\n",
    "# TODO \n",
    "# Jigsaw\n",
    "\n",
    "# Need to change them since they are pandas series objects\n",
    "X_train_toxic = np.array(X_train_toxic, dtype=str)\n",
    "X_val_toxic = np.array(X_val_toxic, dtype=str)\n",
    "X_test_toxic = np.array(X_test_toxic, dtype=str)\n",
    "\n",
    "y_train_toxic = np.array(y_train_toxic, dtype=np.float32)\n",
    "y_val_toxic = np.array(y_val_toxic, dtype=np.float32)\n",
    "y_test_toxic = np.array(y_test_toxic, dtype=np.float32)\n",
    "\n",
    "# Load tokenizer for DeBERTa-v3-base (moved before usage)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "# Get average text size, through all the train and val texts\n",
    "# Fix: Use np.concatenate instead of + operator for numpy arrays\n",
    "sample_texts = np.concatenate([X_train_toxic, X_val_toxic])[:2000]  # first 1000 texts\n",
    "avg_tokens = round(np.mean([len(tokenizer.encode(t, add_special_tokens=True)) for t in sample_texts]))\n",
    "print(avg_tokens)\n",
    "\n",
    "# Tokenization\n",
    "train_encodings = tokenizer(\n",
    "    X_train_toxic.tolist(),  # Convert to list for better compatibility\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=avg_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    X_val_toxic.tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=avg_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    X_test_toxic.tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=avg_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "y_train_toxic_tensor = torch.tensor(y_train_toxic, dtype=torch.float32)\n",
    "y_val_toxic_tensor = torch.tensor(y_val_toxic, dtype=torch.float32)\n",
    "y_test_toxic_tensor = torch.tensor(y_test_toxic, dtype=torch.float32)\n",
    "\n",
    "train_toxic_dataset = TensorDataset(\n",
    "    train_encodings[\"input_ids\"],\n",
    "    train_encodings[\"attention_mask\"],\n",
    "    y_train_toxic_tensor\n",
    ")\n",
    "\n",
    "val_toxic_dataset = TensorDataset(\n",
    "    val_encodings[\"input_ids\"],\n",
    "    val_encodings[\"attention_mask\"],\n",
    "    y_val_toxic_tensor\n",
    ")\n",
    "\n",
    "test_toxic_dataset = TensorDataset(\n",
    "    test_encodings[\"input_ids\"],\n",
    "    test_encodings[\"attention_mask\"],\n",
    "    y_test_toxic_tensor\n",
    ")\n",
    "\n",
    "train_toxic_loader = DataLoader(train_toxic_dataset, batch_size=32, shuffle=True)\n",
    "val_toxic_loader = DataLoader(val_toxic_dataset, batch_size=32, shuffle=False) \n",
    "test_toxic_loader = DataLoader(test_toxic_dataset, batch_size=32, shuffle=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93d0f0",
   "metadata": {},
   "source": [
    "## Model architecture and training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model needs to contain 2 different heads (return values) one for the classification problem\n",
    "# and one for the span\n",
    "class ToxicityModel(nn.Module):\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v3-base\"):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Heads\n",
    "        self.seq_head = nn.Linear(hidden, 1)   # [batch, 1]\n",
    "        self.tok_head = nn.Linear(hidden, 2)   # [batch, seq_len, 2] -> CE over classes\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        out = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden = self.dropout(out.last_hidden_state)   # [B, T, H]\n",
    "        cls_pooled  = self.dropout(last_hidden[:, 0])       # CLS pooling for DeBERTa-v3\n",
    "\n",
    "        seq_logits  = self.seq_head(cls_pooled)             # [B, 1]\n",
    "        tok_logits  = self.tok_head(last_hidden)      \n",
    "              # [B, T, 2]\n",
    "        return seq_logits, tok_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ca0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def freeze_backbone(model, freeze=True):\n",
    "    for p in model.backbone.parameters():\n",
    "        p.requires_grad = not freeze\n",
    "\n",
    "@torch.no_grad()\n",
    "def bin_acc_from_logits(logits, labels):\n",
    "    \"\"\"\n",
    "    logits: [B, 1], raw (pre-sigmoid)\n",
    "    labels: [B, 1] or [B], 0/1\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= 0.5).long()\n",
    "    labs  = labels.view_as(preds).long()\n",
    "    return (preds == labs).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cef39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided by you:\n",
    "# train_toxic_loader = DataLoader(train_toxic_dataset, batch_size=32, shuffle=True)\n",
    "# val_toxic_loader   = DataLoader(val_toxic_dataset,   batch_size=32, shuffle=False)\n",
    "\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = ToxicityModel().to(device)\n",
    "\n",
    "# Train full model on classification first\n",
    "freeze_backbone(model, freeze=False)\n",
    "\n",
    "clf_criterion = BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs_stage1 = 3\n",
    "tr_losses, va_losses = [], []\n",
    "tr_accs,   va_accs   = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs_stage1+1):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc  = 0.0\n",
    "    total_n    = 0\n",
    "\n",
    "    for batch in tqdm(train_toxic_loader, desc=f\"Stage 1 | Epoch {epoch} [train]\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        seq_logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = clf_criterion(seq_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = labels.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc  += bin_acc_from_logits(seq_logits.detach(), labels) * bs\n",
    "        total_n    += bs\n",
    "\n",
    "    tr_losses.append(total_loss / total_n)\n",
    "    tr_accs.append(total_acc / total_n)\n",
    "\n",
    "    # ---- validate ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc  = 0.0\n",
    "    val_n    = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_toxic_loader, desc=f\"Stage 1 | Epoch {epoch} [val]\"):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "\n",
    "            seq_logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = clf_criterion(seq_logits, labels)\n",
    "\n",
    "            bs = labels.size(0)\n",
    "            val_loss += loss.item() * bs\n",
    "            val_acc  += bin_acc_from_logits(seq_logits, labels) * bs\n",
    "            val_n    += bs\n",
    "\n",
    "    va_losses.append(val_loss / val_n)\n",
    "    va_accs.append(val_acc / val_n)\n",
    "\n",
    "    print(f\"[Epoch {epoch}] TrainLoss {tr_losses[-1]:.4f} | TrainAcc {tr_accs[-1]:.4f} | \"\n",
    "          f\"ValLoss {va_losses[-1]:.4f} | ValAcc {va_accs[-1]:.4f}\")\n",
    "\n",
    "# Plot Stage 1 curves\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(1,2,1); plt.plot(tr_losses, label=\"Train\"); plt.plot(va_losses, label=\"Val\")\n",
    "plt.title(\"Stage 1: Loss\"); plt.xlabel(\"Epoch\"); plt.legend()\n",
    "plt.subplot(1,2,2); plt.plot(tr_accs, label=\"Train\"); plt.plot(va_accs, label=\"Val\")\n",
    "plt.title(\"Stage 1: Accuracy\"); plt.xlabel(\"Epoch\"); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save after Stage 1\n",
    "model.save_pretrained(\"toxicity_dualhead_stage1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
