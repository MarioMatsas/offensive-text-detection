{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f88f9c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600cc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import ast\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f82a628",
   "metadata": {},
   "source": [
    "## Proccess data\n",
    "1. **SemEval-2021 Task 5: Toxic Spans Detection**, to detect the offensive part of the message\n",
    "2. **Jigsaw**, to detect toxic messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59878b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for the SemEval-2021 Task 5: Toxic Spans Detection dataset\n",
    "def extract_text_and_positions(dataset_split):\n",
    "    X = []\n",
    "    y = []  # each entry = set of toxic char indices\n",
    "    for sample in dataset_split:\n",
    "        text = sample[\"text_of_post\"]\n",
    "        X.append(text)\n",
    "        try:\n",
    "            toxic_positions = ast.literal_eval(sample[\"position\"])\n",
    "        except:\n",
    "            toxic_positions = []\n",
    "        y.append(set(toxic_positions)) # Not sure if duplicates are present, using sets just to be sure\n",
    "    return X, y\n",
    "\n",
    "def encode_with_labels(texts, toxic_positions_list, tokenizer, max_length):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    for text, toxic_positions in zip(texts, toxic_positions_list):\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        # create token-level labels\n",
    "        token_labels = []\n",
    "        for start, end in encoding[\"offset_mapping\"]:\n",
    "            if start == end:  # special tokens\n",
    "                token_labels.append(-100)\n",
    "            else:\n",
    "                toxic = any(pos in toxic_positions for pos in range(start, end))\n",
    "                token_labels.append(1 if toxic else 0)\n",
    "\n",
    "        input_ids.append(encoding[\"input_ids\"])\n",
    "        attention_masks.append(encoding[\"attention_mask\"])\n",
    "        labels.append(token_labels)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(input_ids, dtype=torch.long),\n",
    "        torch.tensor(attention_masks, dtype=torch.long),\n",
    "        torch.tensor(labels, dtype=torch.long),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7901fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10006\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# SemEval-2021 Task 5: Toxic Spans Detection\n",
    "dataset = load_dataset(\"heegyu/toxic-spans\")\n",
    "train = dataset[\"train\"]\n",
    "test = dataset[\"test\"]\n",
    "X_train_span, y_train_span = extract_text_and_positions(train)\n",
    "X_test_span, y_test_span = extract_text_and_positions(test)\n",
    "# Split to train and val\n",
    "print(len(X_train_span))\n",
    "print(len(X_test_span))\n",
    "X_train_span, X_val_span, y_train_span, y_val_span = train_test_split(\n",
    "    X_train_span, y_train_span, test_size=0.15, random_state=2025\n",
    ")\n",
    "\n",
    "# Jigsaw\n",
    "# -Train / Val-\n",
    "subcategories = [\"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_data = pd.read_csv(\"jigsaw-toxic-comment-data/train.csv\")\n",
    "# If any subcategory is 1, set toxic to 1\n",
    "train_data[\"toxic\"] = train_data[[\"toxic\"] + subcategories].max(axis=1)\n",
    "X_train_toxic = train_data[\"comment_text\"]\n",
    "y_train_toxic = train_data[\"toxic\"]\n",
    "# Split to train and val\n",
    "X_train_toxic, X_val_toxic, y_train_toxic, y_val_toxic = train_test_split(\n",
    "    X_train_toxic, y_train_toxic, test_size=0.15, stratify=y_train_toxic, random_state=2025\n",
    ")\n",
    "\n",
    "# -Test-\n",
    "test_text = pd.read_csv(\"jigsaw-toxic-comment-data/test.csv\")\n",
    "test_labels = pd.read_csv(\"jigsaw-toxic-comment-data/test_labels.csv\")\n",
    "# Keep only rows where toxic is not -1\n",
    "mask = test_labels[\"toxic\"] != -1\n",
    "test_text = test_text[mask].reset_index(drop=True)\n",
    "test_labels = test_labels[mask].reset_index(drop=True)\n",
    "test_labels[\"toxic\"] = test_labels[[\"toxic\"] + subcategories].max(axis=1)\n",
    "X_test_toxic = test_text[\"comment_text\"]\n",
    "y_test_toxic = test_labels[\"toxic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a583ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probability': '{(86, 92): 0.6666666666666666, (8, 13): 0.6666666666666666}',\n",
       " 'position': '[8, 9, 10, 11, 12, 86, 87, 88, 89, 90, 91]',\n",
       " 'text': \"{'stupid': 0.6666666666666666, 'clown': 0.6666666666666666}\",\n",
       " 'type': \"{'insult': 1.0}\",\n",
       " 'support': 3,\n",
       " 'text_of_post': 'Another clown in favour of more tax in this country. Blows my mind people can be this stupid.',\n",
       " 'position_probability': '{86: 0.6666666666666666, 87: 0.6666666666666666, 88: 0.6666666666666666, 89: 0.6666666666666666, 90: 0.6666666666666666, 91: 0.6666666666666666, 8: 0.6666666666666666, 9: 0.6666666666666666, 10: 0.6666666666666666, 11: 0.6666666666666666, 12: 0.6666666666666666}',\n",
       " 'toxic': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "813e7c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'your an idiot life for 10k smh'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_span[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d25607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer for DeBERTa-v3-base (moved before usage)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "avg_tokens = 90\n",
    "\n",
    "# Toxic Spans Detection\n",
    "train_input_ids, train_attention_masks, train_labels = encode_with_labels(\n",
    "    X_train_span, y_train_span, tokenizer, avg_tokens\n",
    ")\n",
    "val_input_ids, val_attention_masks, val_labels = encode_with_labels(\n",
    "    X_val_span, y_val_span, tokenizer, avg_tokens\n",
    ")\n",
    "test_input_ids, test_attention_masks, test_labels = encode_with_labels(\n",
    "    X_test_span, y_test_span, tokenizer, avg_tokens\n",
    ")\n",
    "\n",
    "train_span_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_span_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "test_span_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "train_span_loader = DataLoader(train_span_dataset, batch_size=8, shuffle=True)\n",
    "val_span_loader = DataLoader(val_span_dataset, batch_size=8, shuffle=False)\n",
    "test_span_loader = DataLoader(test_span_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Jigsaw\n",
    "# Need to change them since they are pandas series objects\n",
    "X_train_toxic = np.array(X_train_toxic, dtype=str)\n",
    "X_val_toxic = np.array(X_val_toxic, dtype=str)\n",
    "X_test_toxic = np.array(X_test_toxic, dtype=str)\n",
    "\n",
    "y_train_toxic = np.array(y_train_toxic, dtype=np.float32)\n",
    "y_val_toxic = np.array(y_val_toxic, dtype=np.float32)\n",
    "y_test_toxic = np.array(y_test_toxic, dtype=np.float32)\n",
    "\n",
    "# Tokenization\n",
    "train_encodings = tokenizer(\n",
    "    X_train_toxic.tolist(),  # Convert to list for better compatibility\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=avg_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    X_val_toxic.tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=avg_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    X_test_toxic.tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=avg_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "y_train_toxic_tensor = torch.tensor(y_train_toxic, dtype=torch.float32)\n",
    "y_val_toxic_tensor = torch.tensor(y_val_toxic, dtype=torch.float32)\n",
    "y_test_toxic_tensor = torch.tensor(y_test_toxic, dtype=torch.float32)\n",
    "\n",
    "train_toxic_dataset = TensorDataset(\n",
    "    train_encodings[\"input_ids\"],\n",
    "    train_encodings[\"attention_mask\"],\n",
    "    y_train_toxic_tensor\n",
    ")\n",
    "\n",
    "val_toxic_dataset = TensorDataset(\n",
    "    val_encodings[\"input_ids\"],\n",
    "    val_encodings[\"attention_mask\"],\n",
    "    y_val_toxic_tensor\n",
    ")\n",
    "\n",
    "test_toxic_dataset = TensorDataset(\n",
    "    test_encodings[\"input_ids\"],\n",
    "    test_encodings[\"attention_mask\"],\n",
    "    y_test_toxic_tensor\n",
    ")\n",
    "\n",
    "train_toxic_loader = DataLoader(train_toxic_dataset, batch_size=8, shuffle=True)\n",
    "val_toxic_loader = DataLoader(val_toxic_dataset, batch_size=8, shuffle=False) \n",
    "test_toxic_loader = DataLoader(test_toxic_dataset, batch_size=8, shuffle=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93d0f0",
   "metadata": {},
   "source": [
    "## Model architecture and training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model needs to contain 2 different heads (return values) one for the classification problem\n",
    "# and one for the span\n",
    "class ToxicityModel(nn.Module):\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v3-base\"):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Heads\n",
    "        self.classification_head = nn.Linear(hidden, 1) \n",
    "        # This COULD have been (hidden, 1), since we only have 2 classes but we use softmax because it's the standard way\n",
    "        self.token_head = nn.Linear(hidden, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        out = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden = self.dropout(out.last_hidden_state) \n",
    "        cls_pooled  = self.dropout(last_hidden[:, 0]) # CLS pooling for DeBERTa-v3\n",
    "\n",
    "        clf_logits  = self.classification_head(cls_pooled) # [B, 1]\n",
    "        tok_logits  = self.token_head(last_hidden)      \n",
    "              # [B, T, 2]\n",
    "        return clf_logits, tok_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ca0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def freeze_backbone(model, freeze=True):\n",
    "    for p in model.backbone.parameters():\n",
    "        p.requires_grad = not freeze\n",
    "\n",
    "@torch.no_grad()\n",
    "def bin_acc_from_logits(logits, labels, threshold):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= threshold).long()\n",
    "    labs  = labels.view_as(preds).long()\n",
    "    return (preds == labs).float().mean().item()\n",
    "\n",
    "def find_best_threshold_sequence(model, val_loader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in tqdm(val_loader, desc=\"Val (Clf)\"):\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            clf_logits, _ = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(clf_logits).squeeze(-1)\n",
    "            all_probs.extend(probs.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    thresholds = np.linspace(0.1, 0.9, 17)  # try 0.1, 0.15, ... 0.9\n",
    "    best_thresh, best_f1 = 0.5, 0.0\n",
    "\n",
    "    for t in thresholds:\n",
    "        preds = (np.array(all_probs) >= t).astype(int)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(all_labels, preds, average=\"binary\", zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = t\n",
    "\n",
    "    print(f\"Best threshold (sequence): {best_thresh:.2f} with F1={best_f1:.4f}\")\n",
    "    return best_thresh\n",
    "\n",
    "\n",
    "def find_best_threshold_spans(model, val_loader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in tqdm(val_loader, desc=\"Val (Spans)\"):\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            _, tok_logits = model(input_ids, attention_mask)\n",
    "            tok_probs = F.softmax(tok_logits, dim=-1)[..., 1]  # toxic prob\n",
    "            all_probs.extend(tok_probs.cpu().view(-1).tolist())\n",
    "            all_labels.extend(labels.cpu().view(-1).tolist())\n",
    "\n",
    "    # remove padding (-100) if present\n",
    "    valid_pairs = [(p, l) for p, l in zip(all_probs, all_labels) if l != -100]\n",
    "    all_probs, all_labels = zip(*valid_pairs)\n",
    "\n",
    "    thresholds = np.linspace(0.1, 0.9, 17)\n",
    "    best_thresh, best_f1 = 0.5, 0.0\n",
    "\n",
    "    for t in thresholds:\n",
    "        preds = (np.array(all_probs) >= t).astype(int)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(all_labels, preds, average=\"binary\", zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = t\n",
    "\n",
    "    print(f\"Best threshold (spans): {best_thresh:.2f} with F1={best_f1:.4f}\")\n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cef39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we train on the jigsaw dataset for toxicity detection\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = ToxicityModel().to(device)\n",
    "\n",
    "# Unfreeze backbone for this part of the training\n",
    "freeze_backbone(model, freeze=False)\n",
    "\n",
    "clf_criterion = BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=0.00002)\n",
    "\n",
    "num_epochs_stage1 = 4\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs_stage1+1):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_n = 0\n",
    "\n",
    "    for batch in train_toxic_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        clf_logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = clf_criterion(clf_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = labels.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc += bin_acc_from_logits(clf_logits.detach(), labels) * bs\n",
    "        total_n += bs\n",
    "\n",
    "    train_losses.append(total_loss / total_n)\n",
    "    train_accs.append(total_acc / total_n)\n",
    "\n",
    "    print(\"Train epoch done\")\n",
    "\n",
    "    # ---- validate ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_toxic_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            clf_logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = clf_criterion(clf_logits, labels)\n",
    "\n",
    "            bs = labels.size(0)\n",
    "            val_loss += loss.item() * bs\n",
    "            val_acc += bin_acc_from_logits(clf_logits, labels) * bs\n",
    "            val_n += bs\n",
    "\n",
    "    val_losses.append(val_loss / val_n)\n",
    "    val_accs.append(val_acc / val_n)\n",
    "\n",
    "    print(f\"[Epoch {epoch}] TrainLoss {train_losses[-1]:.4f} | TrainAcc {train_accs[-1]:.4f} | \"\n",
    "          f\"ValLoss {val_losses[-1]:.4f} | ValAcc {val_accs[-1]:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"toxicity_model_epoch{epoch}.pth\")\n",
    "\n",
    "# ---- Run grid search on validation set for final threshold ----\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_toxic_loader, desc=\"Collecting predictions for threshold search\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        clf_logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.sigmoid(clf_logits).cpu().numpy()\n",
    "\n",
    "        all_preds.extend(probs)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "all_preds = np.array(all_preds).flatten()\n",
    "all_labels = np.array(all_labels).flatten()\n",
    "\n",
    "# Search for best threshold based on f1 score\n",
    "thresholds = np.linspace(0.1, 0.9, 81) \n",
    "best_f1, best_thr = 0, 0.5\n",
    "for thr in thresholds:\n",
    "    f1 = f1_score(all_labels, (all_preds >= thr).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thr = f1, thr\n",
    "\n",
    "roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"\\nBest threshold: {best_thr:.2f}\")\n",
    "print(f\"Validation F1 at best threshold: {best_f1:.4f}\")\n",
    "print(f\"Validation ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# ---- Plot Stage 1 curves ----\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(1,2,1); plt.plot(train_losses, label=\"Train\"); plt.plot(val_losses, label=\"Val\")\n",
    "plt.title(\"Stage 1: Loss\"); plt.xlabel(\"Epoch\"); plt.legend()\n",
    "plt.subplot(1,2,2); plt.plot(train_accs, label=\"Train\"); plt.plot(val_accs, label=\"Val\")\n",
    "plt.title(\"Stage 1: Accuracy\"); plt.xlabel(\"Epoch\"); plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855dcd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading after the first part of the training proccess\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ToxicityModel(model_name=\"microsoft/deberta-v3-base\")\n",
    "checkpoint_path = \"models/toxicity_model_final.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f554b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we fine tune even further using the span dataset, due to the large difference in data amount\n",
    "# We also take extra precautions against overfitting, like weight decay and gradient clipping\n",
    "\n",
    "# Start with backbone frozen\n",
    "# The reason we want to do this is because the backbone has already been\n",
    "# trained on the jigsaw data and we don't to ruin the weights. To counter this \n",
    "# we can first train just the head and only unfreeze the backbone, once the head \n",
    "# is already well trained.\n",
    "freeze_backbone(model, freeze=True)\n",
    "\n",
    "tok_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# We set a higher LR for the classification head only\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4, weight_decay=0.01)\n",
    "\n",
    "num_epochs_stage2 = 8\n",
    "unfreeze_epoch = 4  # at this epoch we unfreeze the backbone\n",
    "\n",
    "tr_losses, va_losses = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs_stage2+1):\n",
    "    # Check for epoch to unfreeze the backbone\n",
    "    if epoch == unfreeze_epoch:\n",
    "        freeze_backbone(model, freeze=False)\n",
    "        # Lower LR for full model fine-tuning\n",
    "        optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_n = 0\n",
    "\n",
    "    for batch in train_span_loader:\n",
    "        input_ids, attention_mask, tok_labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        tok_labels = tok_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        _, tok_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = tok_criterion(tok_logits.view(-1, 2), tok_labels.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        bs = input_ids.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_n += bs\n",
    "\n",
    "    tr_losses.append(total_loss / total_n)\n",
    "    print(f\"Train epoch {epoch} done\")\n",
    "\n",
    "    # ---- validate ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_span_loader:\n",
    "            input_ids, attention_mask, tok_labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            tok_labels = tok_labels.to(device)\n",
    "\n",
    "            _, tok_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = tok_criterion(tok_logits.view(-1, 2), tok_labels.view(-1))\n",
    "\n",
    "            bs = input_ids.size(0)\n",
    "            val_loss += loss.item() * bs\n",
    "            val_n += bs\n",
    "\n",
    "    va_losses.append(val_loss / val_n)\n",
    "\n",
    "    print(f\"[Epoch {epoch}] TrainLoss {tr_losses[-1]:.4f} | ValLoss {va_losses[-1]:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), f\"toxicity_span_epoch{epoch}.pth\")\n",
    "\n",
    "# ---- Plot Stage 2 curves ----\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(tr_losses, label=\"Train\")\n",
    "plt.plot(va_losses, label=\"Val\")\n",
    "plt.axvline(x=unfreeze_epoch-1, color='r', linestyle='--', alpha=0.7, label='Unfreeze')\n",
    "plt.title(\"Stage 2: Span Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b20a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading after the second part of the training proccess\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ToxicityModel(model_name=\"microsoft/deberta-v3-base\")\n",
    "checkpoint_path = \"models/toxicity_span_epoch5.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f3568",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3590e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finaly we test the model on our toxicity detection and span detection test data\n",
    "model.eval()\n",
    "\n",
    "t_toxic = find_best_threshold_sequence(model, val_toxic_loader, device)\n",
    "t_span = find_best_threshold_spans(model, val_span_loader, device)\n",
    "\n",
    "# Classification (Jigsaw data)\n",
    "all_preds, all_probs, all_labels = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in tqdm(test_toxic_loader, desc=\"Jigsaw Eval\"):\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        seq_logits, _ = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(seq_logits).squeeze(-1)\n",
    "\n",
    "        preds = (probs >= t_toxic).long()\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_probs.extend(probs.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\")\n",
    "auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "print(\"\\nJigsaw Results:\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {auc:.4f}\")\n",
    "\n",
    "# Token classification (span detection)\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in tqdm(test_span_loader, desc=\"Toxic Spans Eval\"):\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        _, tok_logits = model(input_ids, attention_mask)\n",
    "        tok_probs = F.softmax(tok_logits, dim=-1)[..., 1]  # toxic prob per token\n",
    "        tok_preds = (tok_probs >= t_span).long()\n",
    "\n",
    "        # Flatten across batch and sequence length\n",
    "        all_preds.extend(tok_preds.cpu().view(-1).tolist())\n",
    "        all_labels.extend(labels.cpu().view(-1).tolist())\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\")\n",
    "\n",
    "print(\"\\nToxic Spans Results:\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
