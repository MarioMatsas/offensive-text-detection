{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f88f9c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b600cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import ast\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f82a628",
   "metadata": {},
   "source": [
    "## Proccess data\n",
    "1. **SemEval-2021 Task 5: Toxic Spans Detection**, to detect the offensive part of the message\n",
    "2. **Jigsaw**, to detect toxic messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c59878b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for the SemEval-2021 Task 5: Toxic Spans Detection dataset\n",
    "def extract_text_and_positions(dataset_split):\n",
    "    X = []\n",
    "    y = []  # each entry = set of toxic char indices\n",
    "    for sample in dataset_split:\n",
    "        text = sample[\"text_of_post\"]\n",
    "        X.append(text)\n",
    "        try:\n",
    "            toxic_positions = ast.literal_eval(sample[\"position\"])\n",
    "        except:\n",
    "            toxic_positions = []\n",
    "        y.append(set(toxic_positions))  # store as set for O(1) lookup\n",
    "    return X, y\n",
    "\n",
    "def encode_with_labels(texts, toxic_positions_list, tokenizer, max_length):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    for text, toxic_positions in zip(texts, toxic_positions_list):\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        # create token-level labels\n",
    "        token_labels = []\n",
    "        for start, end in encoding[\"offset_mapping\"]:\n",
    "            if start == end:  # special tokens\n",
    "                token_labels.append(-100)\n",
    "            else:\n",
    "                toxic = any(pos in toxic_positions for pos in range(start, end))\n",
    "                token_labels.append(1 if toxic else 0)\n",
    "\n",
    "        input_ids.append(encoding[\"input_ids\"])\n",
    "        attention_masks.append(encoding[\"attention_mask\"])\n",
    "        labels.append(token_labels)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(input_ids, dtype=torch.long),\n",
    "        torch.tensor(attention_masks, dtype=torch.long),\n",
    "        torch.tensor(labels, dtype=torch.long),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7901fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10006\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# SemEval-2021 Task 5: Toxic Spans Detection\n",
    "dataset = load_dataset(\"heegyu/toxic-spans\")\n",
    "train = dataset[\"train\"]\n",
    "test = dataset[\"test\"]\n",
    "X_train_span, y_train_span = extract_text_and_positions(train)\n",
    "X_test_span, y_test_span = extract_text_and_positions(test)\n",
    "# Split to train and val\n",
    "print(len(X_train_span))\n",
    "print(len(X_test_span))\n",
    "X_train_span, X_val_span, y_train_span, y_val_span = train_test_split(\n",
    "    X_train_span, y_train_span, test_size=0.15, random_state=2025\n",
    ")\n",
    "\n",
    "# Jigsaw\n",
    "# -Train / Val-\n",
    "subcategories = [\"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_data = pd.read_csv(\"jigsaw-toxic-comment-data/train.csv\")\n",
    "# If any subcategory is 1, set toxic to 1\n",
    "train_data[\"toxic\"] = train_data[[\"toxic\"] + subcategories].max(axis=1)\n",
    "X_train_toxic = train_data[\"comment_text\"]\n",
    "y_train_toxic = train_data[\"toxic\"]\n",
    "# Split to train and val\n",
    "X_train_toxic, X_val_toxic, y_train_toxic, y_val_toxic = train_test_split(\n",
    "    X_train_toxic, y_train_toxic, test_size=0.15, stratify=y_train_toxic, random_state=2025\n",
    ")\n",
    "\n",
    "# -Test-\n",
    "test_text = pd.read_csv(\"jigsaw-toxic-comment-data/test.csv\")\n",
    "test_labels = pd.read_csv(\"jigsaw-toxic-comment-data/test_labels.csv\")\n",
    "# Keep only rows where toxic is not -1\n",
    "mask = test_labels[\"toxic\"] != -1\n",
    "test_text = test_text[mask].reset_index(drop=True)\n",
    "test_labels = test_labels[mask].reset_index(drop=True)\n",
    "test_labels[\"toxic\"] = test_labels[[\"toxic\"] + subcategories].max(axis=1)\n",
    "X_test_toxic = test_text[\"comment_text\"]\n",
    "y_test_toxic = test_labels[\"toxic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a583ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probability': '{(86, 92): 0.6666666666666666, (8, 13): 0.6666666666666666}',\n",
       " 'position': '[8, 9, 10, 11, 12, 86, 87, 88, 89, 90, 91]',\n",
       " 'text': \"{'stupid': 0.6666666666666666, 'clown': 0.6666666666666666}\",\n",
       " 'type': \"{'insult': 1.0}\",\n",
       " 'support': 3,\n",
       " 'text_of_post': 'Another clown in favour of more tax in this country. Blows my mind people can be this stupid.',\n",
       " 'position_probability': '{86: 0.6666666666666666, 87: 0.6666666666666666, 88: 0.6666666666666666, 89: 0.6666666666666666, 90: 0.6666666666666666, 91: 0.6666666666666666, 8: 0.6666666666666666, 9: 0.6666666666666666, 10: 0.6666666666666666, 11: 0.6666666666666666, 12: 0.6666666666666666}',\n",
       " 'toxic': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "813e7c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'your an idiot life for 10k smh'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_span[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73d25607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\matsa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer for DeBERTa-v3-base (moved before usage)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "avg_tokens = 90\n",
    "\n",
    "# Toxic Spans Detection\n",
    "train_input_ids, train_attention_masks, train_labels = encode_with_labels(\n",
    "    X_train_span, y_train_span, tokenizer, avg_tokens\n",
    ")\n",
    "val_input_ids, val_attention_masks, val_labels = encode_with_labels(\n",
    "    X_val_span, y_val_span, tokenizer, avg_tokens\n",
    ")\n",
    "test_input_ids, test_attention_masks, test_labels = encode_with_labels(\n",
    "    X_test_span, y_test_span, tokenizer, avg_tokens\n",
    ")\n",
    "\n",
    "train_span_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_span_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "test_span_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "train_span_loader = DataLoader(train_span_dataset, batch_size=8, shuffle=True)\n",
    "val_span_loader = DataLoader(val_span_dataset, batch_size=8, shuffle=False)\n",
    "test_span_loader = DataLoader(test_span_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Jigsaw\n",
    "# Need to change them since they are pandas series objects\n",
    "X_train_toxic = np.array(X_train_toxic, dtype=str)\n",
    "X_val_toxic = np.array(X_val_toxic, dtype=str)\n",
    "X_test_toxic = np.array(X_test_toxic, dtype=str)\n",
    "\n",
    "y_train_toxic = np.array(y_train_toxic, dtype=np.float32)\n",
    "y_val_toxic = np.array(y_val_toxic, dtype=np.float32)\n",
    "y_test_toxic = np.array(y_test_toxic, dtype=np.float32)\n",
    "\n",
    "# Tokenization\n",
    "train_encodings = tokenizer(\n",
    "    X_train_toxic.tolist(),  # Convert to list for better compatibility\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=avg_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    X_val_toxic.tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=avg_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    X_test_toxic.tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=avg_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "y_train_toxic_tensor = torch.tensor(y_train_toxic, dtype=torch.float32)\n",
    "y_val_toxic_tensor = torch.tensor(y_val_toxic, dtype=torch.float32)\n",
    "y_test_toxic_tensor = torch.tensor(y_test_toxic, dtype=torch.float32)\n",
    "\n",
    "train_toxic_dataset = TensorDataset(\n",
    "    train_encodings[\"input_ids\"],\n",
    "    train_encodings[\"attention_mask\"],\n",
    "    y_train_toxic_tensor\n",
    ")\n",
    "\n",
    "val_toxic_dataset = TensorDataset(\n",
    "    val_encodings[\"input_ids\"],\n",
    "    val_encodings[\"attention_mask\"],\n",
    "    y_val_toxic_tensor\n",
    ")\n",
    "\n",
    "test_toxic_dataset = TensorDataset(\n",
    "    test_encodings[\"input_ids\"],\n",
    "    test_encodings[\"attention_mask\"],\n",
    "    y_test_toxic_tensor\n",
    ")\n",
    "\n",
    "train_toxic_loader = DataLoader(train_toxic_dataset, batch_size=8, shuffle=True)\n",
    "val_toxic_loader = DataLoader(val_toxic_dataset, batch_size=8, shuffle=False) \n",
    "test_toxic_loader = DataLoader(test_toxic_dataset, batch_size=8, shuffle=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93d0f0",
   "metadata": {},
   "source": [
    "## Model architecture and training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model needs to contain 2 different heads (return values) one for the classification problem\n",
    "# and one for the span\n",
    "class ToxicityModel(nn.Module):\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v3-base\"):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Heads\n",
    "        self.seq_head = nn.Linear(hidden, 1)   # [batch, 1]\n",
    "        self.tok_head = nn.Linear(hidden, 2)   # [batch, seq_len, 2] -> CE over classes\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        out = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden = self.dropout(out.last_hidden_state)   # [B, T, H]\n",
    "        cls_pooled  = self.dropout(last_hidden[:, 0])       # CLS pooling for DeBERTa-v3\n",
    "\n",
    "        seq_logits  = self.seq_head(cls_pooled)             # [B, 1]\n",
    "        tok_logits  = self.tok_head(last_hidden)      \n",
    "              # [B, T, 2]\n",
    "        return seq_logits, tok_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ca0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def freeze_backbone(model, freeze=True):\n",
    "    for p in model.backbone.parameters():\n",
    "        p.requires_grad = not freeze\n",
    "\n",
    "@torch.no_grad()\n",
    "def bin_acc_from_logits(logits, labels, threshold):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= threshold).long()\n",
    "    labs  = labels.view_as(preds).long()\n",
    "    return (preds == labs).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cef39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we train on the jigsaw dataset for toxicity detection\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = ToxicityModel().to(device)\n",
    "\n",
    "# Unfreeze backbone for Stage 1\n",
    "freeze_backbone(model, freeze=False)\n",
    "\n",
    "clf_criterion = BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs_stage1 = 4\n",
    "tr_losses, va_losses = [], []\n",
    "tr_accs, va_accs     = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs_stage1+1):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc  = 0.0\n",
    "    total_n    = 0\n",
    "\n",
    "    for batch in train_toxic_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        seq_logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = clf_criterion(seq_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = labels.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc  += bin_acc_from_logits(seq_logits.detach(), labels) * bs\n",
    "        total_n    += bs\n",
    "\n",
    "    tr_losses.append(total_loss / total_n)\n",
    "    tr_accs.append(total_acc / total_n)\n",
    "\n",
    "    print(\"Train epoch done\")\n",
    "\n",
    "    # ---- validate ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc  = 0.0\n",
    "    val_n    = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_toxic_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            seq_logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = clf_criterion(seq_logits, labels)\n",
    "\n",
    "            bs = labels.size(0)\n",
    "            val_loss += loss.item() * bs\n",
    "            val_acc  += bin_acc_from_logits(seq_logits, labels) * bs\n",
    "            val_n    += bs\n",
    "\n",
    "    va_losses.append(val_loss / val_n)\n",
    "    va_accs.append(val_acc / val_n)\n",
    "\n",
    "    print(f\"[Epoch {epoch}] TrainLoss {tr_losses[-1]:.4f} | TrainAcc {tr_accs[-1]:.4f} | \"\n",
    "          f\"ValLoss {va_losses[-1]:.4f} | ValAcc {va_accs[-1]:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"toxicity_model_epoch{epoch}.pth\")\n",
    "\n",
    "# ---- Run grid search on validation set for final threshold ----\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_toxic_loader, desc=\"Collecting predictions for threshold search\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        seq_logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.sigmoid(seq_logits).cpu().numpy()\n",
    "\n",
    "        all_preds.extend(probs)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "all_preds = np.array(all_preds).flatten()\n",
    "all_labels = np.array(all_labels).flatten()\n",
    "\n",
    "# Search for best threshold\n",
    "thresholds = np.linspace(0.1, 0.9, 81) \n",
    "best_f1, best_thr = 0, 0.5\n",
    "for thr in thresholds:\n",
    "    f1 = f1_score(all_labels, (all_preds >= thr).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thr = f1, thr\n",
    "\n",
    "roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"\\nBest threshold: {best_thr:.2f}\")\n",
    "print(f\"Validation F1 at best threshold: {best_f1:.4f}\")\n",
    "print(f\"Validation ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# ---- Plot Stage 1 curves ----\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(1,2,1); plt.plot(tr_losses, label=\"Train\"); plt.plot(va_losses, label=\"Val\")\n",
    "plt.title(\"Stage 1: Loss\"); plt.xlabel(\"Epoch\"); plt.legend()\n",
    "plt.subplot(1,2,2); plt.plot(tr_accs, label=\"Train\"); plt.plot(va_accs, label=\"Val\")\n",
    "plt.title(\"Stage 1: Accuracy\"); plt.xlabel(\"Epoch\"); plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855dcd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading after the first part of the training proccess\n",
    "model = ToxicityModel(model_name=\"microsoft/deberta-v3-base\")\n",
    "checkpoint_path = \"models/toxicity_model_final.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")  # \"cuda\"\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f554b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we fine tune even further using the span dataset, due to the large difference in data amount\n",
    "# Start with backbone frozen (Stage 2a)\n",
    "freeze_backbone(model, freeze=True)\n",
    "\n",
    "tok_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Stage 2a: Higher LR for classification head only\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4, weight_decay=0.01)\n",
    "\n",
    "num_epochs_stage2 = 8\n",
    "unfreeze_epoch = 4  # at this epoch we unfreeze backbone (Stage 2b)\n",
    "\n",
    "tr_losses, va_losses = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs_stage2+1):\n",
    "    # ---- Unfreeze backbone at Stage 2b ----\n",
    "    if epoch == unfreeze_epoch:\n",
    "        freeze_backbone(model, freeze=False)\n",
    "        # Stage 2b: Lower LR for full model fine-tuning\n",
    "        optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_n    = 0\n",
    "\n",
    "    for batch in train_span_loader:\n",
    "        input_ids, attention_mask, tok_labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        tok_labels = tok_labels.to(device)   # [B, T], values in {0,1} or -100 for ignore\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        _, tok_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # tok_logits: [B, T, 2]\n",
    "\n",
    "        loss = tok_criterion(tok_logits.view(-1, 2), tok_labels.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        bs = input_ids.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_n    += bs\n",
    "\n",
    "    tr_losses.append(total_loss / total_n)\n",
    "    print(f\"Train epoch {epoch} done\")\n",
    "\n",
    "    # ---- validate ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_n    = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_span_loader:\n",
    "            input_ids, attention_mask, tok_labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            tok_labels = tok_labels.to(device)\n",
    "\n",
    "            _, tok_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = tok_criterion(tok_logits.view(-1, 2), tok_labels.view(-1))\n",
    "\n",
    "            bs = input_ids.size(0)\n",
    "            val_loss += loss.item() * bs\n",
    "            val_n    += bs\n",
    "\n",
    "    va_losses.append(val_loss / val_n)\n",
    "\n",
    "    print(f\"[Epoch {epoch}] TrainLoss {tr_losses[-1]:.4f} | ValLoss {va_losses[-1]:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), f\"toxicity_span_epoch{epoch}.pth\")\n",
    "\n",
    "# ---- Plot Stage 2 curves ----\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(tr_losses, label=\"Train\")\n",
    "plt.plot(va_losses, label=\"Val\")\n",
    "plt.axvline(x=unfreeze_epoch-1, color='r', linestyle='--', alpha=0.7, label='Unfreeze')\n",
    "plt.title(\"Stage 2: Span Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f3568",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3590e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finaly we test the model on our toxicity detection and span detection test data\n",
    "# Toxicity\n",
    "\n",
    "# Span"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
